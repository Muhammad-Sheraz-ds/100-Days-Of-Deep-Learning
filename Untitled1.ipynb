{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09a1502-440f-4903-98ac-4aa578123a20",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "\n",
    "\n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Deep Learning</h1>\n",
    "\n",
    "---\n",
    "<h3 align=\"right\">Muhammad Sheraz (Data Scientist)</h3>\n",
    "<h1 align=\"center\">Day 62 (Attention Mechanism)</h1>\n",
    "<img  src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pem3HpwVeBa8WlzSORzN-Q.jpeg\" alt=\"Descriptive Alt Text\">\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4493b-dda2-4c48-8b19-81e1d3eb4c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9294d9f-4a26-45cb-8b31-0f9d388fb9ad",
   "metadata": {},
   "source": [
    "## Read Documentation related to Attention Mechanism\n",
    "\n",
    "- <h3><a href=\"https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392\" target=\"_blank\">Attention Mechanism in Deep Learning</a></h3>\n",
    "\n",
    "- <h3><a href=\"https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392\" target=\"_blank\">The Attention Mechanism from Scratch</a></h3>\n",
    "\n",
    "- <h3><a href=\"https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392\" target=\"_blank\">Attention Mechanism</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f623e7-0477-4257-ae4e-749c85fbe349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c9d68c-3e44-4a21-a161-bb74aaf2d212",
   "metadata": {},
   "source": [
    "## Why Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72d1df-269f-42af-b333-69e686007373",
   "metadata": {},
   "source": [
    "Encoder processes the input sequence and encodes/compresses/summarizes the information into a context vector (also called as the “thought vector”) of a fixed length. This representation is expected to be a good summary of the entire input sequence. The decoder is then initialized with this context vector, using which it starts generating the transformed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ac9b0-8503-47b3-a493-86864a7a1732",
   "metadata": {},
   "source": [
    "> **A critical and apparent disadvantage of this fixed-length context vector design is the incapability of the system to remember longer sequences. Often is has forgotten the earlier parts of the sequence once it has processed the entire the sequence. The attention mechanism was born to resolve this problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98e977-48a3-4654-8048-2ffc6c95e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6184efc1-d4fe-496a-8084-6fb7ea2b4ef9",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "- An attention mechanism is an Encoder-Decoder kind of neural network architecture that allows the model to focus on specific sections of the input while executing a task.\n",
    "- It dynamically assigns weights to different elements in the input, indicating their relative importance or relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171fe90-6b81-4e15-bace-9338f2225f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8fb9-cec2-4d97-804e-7971d95e8fe3",
   "metadata": {},
   "source": [
    "## Why the name Attention?\n",
    "\n",
    "Imagine you are translating “Rahul is a good boy” to “راحل ایک اچھا لڑکا ہے”. How do you do it in your mind?\n",
    "\n",
    "When you predict “راحل”, it's obvious that this name is the result of the word “Rahul” present in the input English sentence regardless of the rest of the sentence. We say that while predicting “راحل”, we pay more attention to the word “Rahul” in the input sentence.\n",
    "\n",
    "Similarly, while predicting the word “اچھا”, we pay more attention to the word “good” in the input sentence.\n",
    "\n",
    "Similarly, while predicting the word “لڑکا”, we pay more attention to the word “boy” in the input sentence. And so on...\n",
    "\n",
    "Hence the name “ATTENTION”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd47e2-483e-497a-a97d-0afc7777b3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f6684-e2d0-4f5b-ba0d-12fdde5eda73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6fd3e2f-408a-465c-a2af-dfdf0bb82e6b",
   "metadata": {},
   "source": [
    "## How does Attention work?\n",
    "\n",
    "<img  src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*K9NXW3w7O_1p5sRm6NnEuQ.jpeg\" alt=\"Descriptive Alt Text\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1fa7e-cbc5-4c06-b95a-72d8a9759b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
