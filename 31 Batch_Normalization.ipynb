{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "\n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Deep Learning</h1>\n",
    "\n",
    "---\n",
    "<h3 align=\"right\">Muhammad Sheraz (Data Scientist)</h3>\n",
    "<h1 align=\"center\">Day26 (Batch Normalization)</h1>\n",
    "\n",
    "  <div align=\"center\">\n",
    "      <img width=\"45%\" src=\"Images/batch_norm_3.png\">\n",
    "      <img width=\"54%\" src='Images/batch_norm_2.png'>\n",
    "  \n",
    " \n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "- Normalization technique used in deep learning to improve the training `speed` and `performance` of neural networks.\n",
    "- It normalizes the input of each layer by adjusting and scaling the activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it work?\n",
    "\n",
    "- Computes the `mean` and `variance` of each mini-batch during training.\n",
    "- Normalizes the activations using the mean and variance.\n",
    "- Applies `scaling` and `shifting` to the normalized activations using learned parameters (`gamma` and `beta`).\n",
    "- These parameters are `learned` during training via backpropagation.\n",
    "\n",
    "\n",
    "<img src='Images/batcch_work.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Batch Normalization\n",
    "- Helps alleviate issues like vanishing/exploding gradients during training.\n",
    "- Allows for faster convergence by stabilizing the learning process.\n",
    "- Reduces the sensitivity to the initialization of model parameters.\n",
    "- Acts as a regularizer, reducing the need for other regularization techniques.\n",
    "\n",
    "### Drawbacks of Batch Normalization\n",
    "- Increases computational overhead during training due to additional computations.\n",
    "- Makes the model less robust to small batches or non-i.i.d. (independent and identically distributed) data.\n",
    "- Not always suitable for certain types of networks or tasks (e.g., recurrent neural networks).\n",
    "\n",
    "### When to Use Batch Normalization?\n",
    "- Typically used in feedforward neural networks, especially in deep architectures.\n",
    "- Effective when training deep networks with multiple layers.\n",
    "- Generally not used in certain architectures like recurrent neural networks (RNNs) due to the sequential nature of data processing.\n",
    "\n",
    "### Implementation Details\n",
    "- Batch Normalization layers are usually added after the linear transformation and before the activation function in a neural network.\n",
    "- Parameters (`gamma` and `beta`) are learned during training along with other network parameters.\n",
    "- During inference, batch statistics may be calculated differently (e.g., using moving averages) for stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sSXoG0koPOjy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8bWCokiaV_et"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Datasets/concertriccir2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jcOI0Ig6WNQA",
    "outputId": "8e6790d4-b187-4948-fbbb-054f5b803f27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7.003345706103683010e-01</th>\n",
       "      <th>-2.470675778972781789e-01</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.950019</td>\n",
       "      <td>2.740080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150222</td>\n",
       "      <td>-2.157638</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.672050</td>\n",
       "      <td>-0.941519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.560483</td>\n",
       "      <td>-1.846577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.724979</td>\n",
       "      <td>3.463930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   7.003345706103683010e-01  -2.470675778972781789e-01  \\\n",
       "0                 -3.950019                   2.740080   \n",
       "1                  0.150222                  -2.157638   \n",
       "2                 -1.672050                  -0.941519   \n",
       "3                  2.560483                  -1.846577   \n",
       "4                 -1.724979                   3.463930   \n",
       "\n",
       "   0.000000000000000000e+00  \n",
       "0                       1.0  \n",
       "1                       1.0  \n",
       "2                       1.0  \n",
       "3                       1.0  \n",
       "4                       1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u-C3KtY9WOIf"
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=({'7.003345706103683010e-01':'X'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AQf4Ja9GWdnv"
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=({'-2.470675778972781789e-01':'Y'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bokJWOwtWik_"
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=({'0.000000000000000000e+00':'class'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TsW8OagkWnIu",
    "outputId": "1c9d84dc-75a4-4090-dd16-a999566d6ca5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.950019</td>\n",
       "      <td>2.740080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150222</td>\n",
       "      <td>-2.157638</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.672050</td>\n",
       "      <td>-0.941519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.560483</td>\n",
       "      <td>-1.846577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.724979</td>\n",
       "      <td>3.463930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y  class\n",
       "0 -3.950019  2.740080    1.0\n",
       "1  0.150222 -2.157638    1.0\n",
       "2 -1.672050 -0.941519    1.0\n",
       "3  2.560483 -1.846577    1.0\n",
       "4 -1.724979  3.463930    1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "6aVoGPi4WoRO",
    "outputId": "bb7b0d35-522c-48d1-f6cb-dbd9bfdf930e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['X'],df['Y'],c=df['class']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "42B4oLguWqDv"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:2].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fZoqJmrEWsRu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrW9anh5Wuov",
    "outputId": "6ab02eee-52f3-42ce-ae04-da8a11121bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15 (60.00 Byte)\n",
      "Trainable params: 15 (60.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2,activation='relu',input_dim=2))\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VuTwhNMIWwZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbnTruBoWygu",
    "outputId": "609de1b1-88a8-4c79-c0f9-533a760369bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "13/13 [==============================] - 3s 44ms/step - loss: 0.7001 - accuracy: 0.4662 - val_loss: 0.6932 - val_accuracy: 0.4300\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6994 - accuracy: 0.5238 - val_loss: 0.6934 - val_accuracy: 0.4300\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6988 - accuracy: 0.5213 - val_loss: 0.6936 - val_accuracy: 0.4300\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6983 - accuracy: 0.5188 - val_loss: 0.6938 - val_accuracy: 0.4300\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6979 - accuracy: 0.5188 - val_loss: 0.6941 - val_accuracy: 0.4300\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6975 - accuracy: 0.5163 - val_loss: 0.6943 - val_accuracy: 0.4300\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6971 - accuracy: 0.5113 - val_loss: 0.6946 - val_accuracy: 0.4300\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6967 - accuracy: 0.5138 - val_loss: 0.6948 - val_accuracy: 0.4300\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6964 - accuracy: 0.5113 - val_loss: 0.6951 - val_accuracy: 0.4300\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6962 - accuracy: 0.5063 - val_loss: 0.6952 - val_accuracy: 0.4300\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6959 - accuracy: 0.5013 - val_loss: 0.6953 - val_accuracy: 0.4300\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6957 - accuracy: 0.4937 - val_loss: 0.6954 - val_accuracy: 0.4300\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6956 - accuracy: 0.4937 - val_loss: 0.6956 - val_accuracy: 0.4300\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6953 - accuracy: 0.4962 - val_loss: 0.6956 - val_accuracy: 0.4300\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6952 - accuracy: 0.4962 - val_loss: 0.6959 - val_accuracy: 0.4300\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6950 - accuracy: 0.4937 - val_loss: 0.6959 - val_accuracy: 0.4300\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6949 - accuracy: 0.4862 - val_loss: 0.6962 - val_accuracy: 0.4300\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6947 - accuracy: 0.4862 - val_loss: 0.6963 - val_accuracy: 0.4300\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6945 - accuracy: 0.4887 - val_loss: 0.6963 - val_accuracy: 0.4300\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6944 - accuracy: 0.4812 - val_loss: 0.6964 - val_accuracy: 0.4300\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6943 - accuracy: 0.4812 - val_loss: 0.6965 - val_accuracy: 0.4300\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6942 - accuracy: 0.4812 - val_loss: 0.6965 - val_accuracy: 0.4300\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6941 - accuracy: 0.4862 - val_loss: 0.6966 - val_accuracy: 0.4300\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6940 - accuracy: 0.4837 - val_loss: 0.6968 - val_accuracy: 0.4300\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6939 - accuracy: 0.4787 - val_loss: 0.6970 - val_accuracy: 0.4300\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6938 - accuracy: 0.4812 - val_loss: 0.6970 - val_accuracy: 0.4300\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6937 - accuracy: 0.4762 - val_loss: 0.6970 - val_accuracy: 0.4300\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6936 - accuracy: 0.4812 - val_loss: 0.6971 - val_accuracy: 0.4300\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6935 - accuracy: 0.4837 - val_loss: 0.6971 - val_accuracy: 0.4300\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6935 - accuracy: 0.4912 - val_loss: 0.6971 - val_accuracy: 0.4300\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6934 - accuracy: 0.4987 - val_loss: 0.6973 - val_accuracy: 0.4300\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6973 - val_accuracy: 0.4300\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.5088 - val_loss: 0.6976 - val_accuracy: 0.4300\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.4300\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.4300\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5138 - val_loss: 0.6977 - val_accuracy: 0.4300\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5138 - val_loss: 0.6976 - val_accuracy: 0.4300\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5138 - val_loss: 0.6977 - val_accuracy: 0.4300\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6930 - accuracy: 0.5138 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5138 - val_loss: 0.6977 - val_accuracy: 0.4300\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6929 - accuracy: 0.5163 - val_loss: 0.6980 - val_accuracy: 0.4300\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5163 - val_loss: 0.6979 - val_accuracy: 0.4300\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6979 - val_accuracy: 0.4300\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6980 - val_accuracy: 0.4300\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6979 - val_accuracy: 0.4300\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6923 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5188 - val_loss: 0.6986 - val_accuracy: 0.4300\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6921 - accuracy: 0.5188 - val_loss: 0.6986 - val_accuracy: 0.4300\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6917 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6916 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6915 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6986 - val_accuracy: 0.4300\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6911 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6910 - accuracy: 0.5188 - val_loss: 0.6986 - val_accuracy: 0.4300\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6908 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6907 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6905 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6904 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6902 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6898 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6896 - accuracy: 0.5188 - val_loss: 0.6986 - val_accuracy: 0.4300\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6894 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4300\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6892 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6889 - accuracy: 0.5188 - val_loss: 0.6984 - val_accuracy: 0.4300\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6886 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6883 - accuracy: 0.5188 - val_loss: 0.6983 - val_accuracy: 0.4300\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6880 - accuracy: 0.5188 - val_loss: 0.6982 - val_accuracy: 0.4300\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6873 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6869 - accuracy: 0.5188 - val_loss: 0.6981 - val_accuracy: 0.4300\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6865 - accuracy: 0.5188 - val_loss: 0.6980 - val_accuracy: 0.4300\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6860 - accuracy: 0.5188 - val_loss: 0.6979 - val_accuracy: 0.4300\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6856 - accuracy: 0.5188 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6852 - accuracy: 0.5188 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6847 - accuracy: 0.5188 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6843 - accuracy: 0.5188 - val_loss: 0.6978 - val_accuracy: 0.4300\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6836 - accuracy: 0.5188 - val_loss: 0.6976 - val_accuracy: 0.4300\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6831 - accuracy: 0.5188 - val_loss: 0.6974 - val_accuracy: 0.4300\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6827 - accuracy: 0.5188 - val_loss: 0.6974 - val_accuracy: 0.4300\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6822 - accuracy: 0.5188 - val_loss: 0.6973 - val_accuracy: 0.4300\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6819 - accuracy: 0.5188 - val_loss: 0.6972 - val_accuracy: 0.4300\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6814 - accuracy: 0.5188 - val_loss: 0.6971 - val_accuracy: 0.4300\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6810 - accuracy: 0.5188 - val_loss: 0.6971 - val_accuracy: 0.4300\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6806 - accuracy: 0.5188 - val_loss: 0.6969 - val_accuracy: 0.4300\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6803 - accuracy: 0.5188 - val_loss: 0.6965 - val_accuracy: 0.4300\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.6799 - accuracy: 0.5188 - val_loss: 0.6965 - val_accuracy: 0.4300\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6796 - accuracy: 0.5188 - val_loss: 0.6965 - val_accuracy: 0.4300\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6791 - accuracy: 0.5188 - val_loss: 0.6964 - val_accuracy: 0.4300\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6788 - accuracy: 0.5188 - val_loss: 0.6962 - val_accuracy: 0.4300\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6785 - accuracy: 0.5188 - val_loss: 0.6961 - val_accuracy: 0.4300\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6781 - accuracy: 0.5188 - val_loss: 0.6959 - val_accuracy: 0.4300\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6778 - accuracy: 0.5188 - val_loss: 0.6958 - val_accuracy: 0.4300\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6775 - accuracy: 0.5188 - val_loss: 0.6957 - val_accuracy: 0.4300\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6772 - accuracy: 0.5188 - val_loss: 0.6958 - val_accuracy: 0.4300\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6770 - accuracy: 0.5188 - val_loss: 0.6956 - val_accuracy: 0.4300\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6766 - accuracy: 0.5188 - val_loss: 0.6955 - val_accuracy: 0.4300\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6764 - accuracy: 0.5188 - val_loss: 0.6954 - val_accuracy: 0.4300\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6762 - accuracy: 0.5188 - val_loss: 0.6953 - val_accuracy: 0.4300\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6760 - accuracy: 0.5188 - val_loss: 0.6951 - val_accuracy: 0.4300\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6758 - accuracy: 0.5188 - val_loss: 0.6949 - val_accuracy: 0.4300\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.6755 - accuracy: 0.5188 - val_loss: 0.6947 - val_accuracy: 0.4300\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6752 - accuracy: 0.5188 - val_loss: 0.6945 - val_accuracy: 0.4300\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6750 - accuracy: 0.5188 - val_loss: 0.6944 - val_accuracy: 0.4300\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6748 - accuracy: 0.5188 - val_loss: 0.6944 - val_accuracy: 0.4300\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6746 - accuracy: 0.5188 - val_loss: 0.6943 - val_accuracy: 0.4300\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6744 - accuracy: 0.5188 - val_loss: 0.6942 - val_accuracy: 0.4300\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6742 - accuracy: 0.5188 - val_loss: 0.6940 - val_accuracy: 0.4300\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6740 - accuracy: 0.5188 - val_loss: 0.6939 - val_accuracy: 0.4300\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6738 - accuracy: 0.5188 - val_loss: 0.6940 - val_accuracy: 0.4300\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6737 - accuracy: 0.5188 - val_loss: 0.6939 - val_accuracy: 0.4300\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6735 - accuracy: 0.5188 - val_loss: 0.6937 - val_accuracy: 0.4300\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6733 - accuracy: 0.5188 - val_loss: 0.6937 - val_accuracy: 0.4300\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6731 - accuracy: 0.5188 - val_loss: 0.6936 - val_accuracy: 0.4300\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6729 - accuracy: 0.5188 - val_loss: 0.6935 - val_accuracy: 0.4300\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6728 - accuracy: 0.5188 - val_loss: 0.6933 - val_accuracy: 0.4300\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6726 - accuracy: 0.5188 - val_loss: 0.6933 - val_accuracy: 0.4300\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6724 - accuracy: 0.5188 - val_loss: 0.6932 - val_accuracy: 0.4300\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6723 - accuracy: 0.5188 - val_loss: 0.6932 - val_accuracy: 0.4300\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6722 - accuracy: 0.5238 - val_loss: 0.6931 - val_accuracy: 0.5700\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6720 - accuracy: 0.5163 - val_loss: 0.6931 - val_accuracy: 0.5700\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6719 - accuracy: 0.5138 - val_loss: 0.6930 - val_accuracy: 0.5700\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6717 - accuracy: 0.5138 - val_loss: 0.6930 - val_accuracy: 0.5700\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6716 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5700\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6714 - accuracy: 0.5138 - val_loss: 0.6927 - val_accuracy: 0.5700\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6712 - accuracy: 0.5138 - val_loss: 0.6927 - val_accuracy: 0.5700\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6711 - accuracy: 0.5138 - val_loss: 0.6925 - val_accuracy: 0.5700\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6709 - accuracy: 0.5138 - val_loss: 0.6923 - val_accuracy: 0.5700\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6707 - accuracy: 0.5138 - val_loss: 0.6923 - val_accuracy: 0.5700\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6705 - accuracy: 0.5138 - val_loss: 0.6922 - val_accuracy: 0.5700\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6704 - accuracy: 0.5138 - val_loss: 0.6922 - val_accuracy: 0.5700\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6702 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5700\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6701 - accuracy: 0.5213 - val_loss: 0.6921 - val_accuracy: 0.5700\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6699 - accuracy: 0.5213 - val_loss: 0.6920 - val_accuracy: 0.5700\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6698 - accuracy: 0.5213 - val_loss: 0.6921 - val_accuracy: 0.5700\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6696 - accuracy: 0.5213 - val_loss: 0.6919 - val_accuracy: 0.5700\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6696 - accuracy: 0.5213 - val_loss: 0.6919 - val_accuracy: 0.5700\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6694 - accuracy: 0.5213 - val_loss: 0.6919 - val_accuracy: 0.5700\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6693 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5700\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6691 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5700\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6690 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5700\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6690 - accuracy: 0.5213 - val_loss: 0.6917 - val_accuracy: 0.5700\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6688 - accuracy: 0.5213 - val_loss: 0.6916 - val_accuracy: 0.5700\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6687 - accuracy: 0.5213 - val_loss: 0.6917 - val_accuracy: 0.5700\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6686 - accuracy: 0.5213 - val_loss: 0.6916 - val_accuracy: 0.5700\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6685 - accuracy: 0.5213 - val_loss: 0.6915 - val_accuracy: 0.5700\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6684 - accuracy: 0.5238 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6682 - accuracy: 0.5238 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6681 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6681 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6679 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6678 - accuracy: 0.5238 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6678 - accuracy: 0.5238 - val_loss: 0.6913 - val_accuracy: 0.5700\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6677 - accuracy: 0.5238 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6676 - accuracy: 0.5238 - val_loss: 0.6914 - val_accuracy: 0.5700\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6675 - accuracy: 0.5238 - val_loss: 0.6913 - val_accuracy: 0.5700\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6674 - accuracy: 0.5238 - val_loss: 0.6913 - val_accuracy: 0.5700\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6673 - accuracy: 0.5238 - val_loss: 0.6913 - val_accuracy: 0.5700\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6673 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6672 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5700\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6671 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6670 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6669 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6668 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6667 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6666 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5700\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6666 - accuracy: 0.5238 - val_loss: 0.6911 - val_accuracy: 0.5700\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6665 - accuracy: 0.5238 - val_loss: 0.6911 - val_accuracy: 0.5700\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit(X,y,epochs=200,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF2XHk88W0Wu",
    "outputId": "32f0a94e-082e-449e-df73-3dda1dcb4e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 3)                 9         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 3)                 12        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 8         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 2)                 8         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40 (160.00 Byte)\n",
      "Trainable params: 30 (120.00 Byte)\n",
      "Non-trainable params: 10 (40.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(3,activation='relu',input_dim=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Uz5FmY5DXNGe"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7qJmdDbXQYe",
    "outputId": "c22534cf-02c8-43d5-eed3-0adfa3708771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 5s 50ms/step - loss: 0.7443 - accuracy: 0.5138 - val_loss: 0.8154 - val_accuracy: 0.4500\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7398 - accuracy: 0.5013 - val_loss: 0.7822 - val_accuracy: 0.4100\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7302 - accuracy: 0.5388 - val_loss: 0.7484 - val_accuracy: 0.4100\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.7268 - accuracy: 0.5414 - val_loss: 0.7250 - val_accuracy: 0.4200\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7183 - accuracy: 0.5789 - val_loss: 0.7049 - val_accuracy: 0.4000\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7094 - accuracy: 0.5714 - val_loss: 0.6878 - val_accuracy: 0.4900\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6926 - accuracy: 0.5890 - val_loss: 0.6773 - val_accuracy: 0.5100\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6883 - accuracy: 0.6090 - val_loss: 0.6643 - val_accuracy: 0.5200\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6791 - accuracy: 0.6366 - val_loss: 0.6541 - val_accuracy: 0.5500\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6693 - accuracy: 0.6316 - val_loss: 0.6450 - val_accuracy: 0.5800\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6655 - accuracy: 0.6416 - val_loss: 0.6375 - val_accuracy: 0.6000\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6719 - accuracy: 0.6316 - val_loss: 0.6307 - val_accuracy: 0.6300\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6550 - accuracy: 0.6491 - val_loss: 0.6238 - val_accuracy: 0.6800\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6474 - accuracy: 0.6341 - val_loss: 0.6182 - val_accuracy: 0.7100\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6499 - accuracy: 0.6516 - val_loss: 0.6119 - val_accuracy: 0.7000\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6350 - accuracy: 0.6617 - val_loss: 0.6089 - val_accuracy: 0.7000\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6157 - accuracy: 0.7093 - val_loss: 0.6067 - val_accuracy: 0.6900\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6256 - accuracy: 0.6566 - val_loss: 0.6048 - val_accuracy: 0.7000\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6269 - accuracy: 0.6692 - val_loss: 0.6022 - val_accuracy: 0.7200\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6112 - accuracy: 0.7018 - val_loss: 0.5992 - val_accuracy: 0.7200\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6100 - accuracy: 0.6942 - val_loss: 0.5980 - val_accuracy: 0.7300\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6023 - accuracy: 0.6842 - val_loss: 0.5965 - val_accuracy: 0.7300\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6061 - accuracy: 0.6867 - val_loss: 0.5955 - val_accuracy: 0.7200\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5950 - accuracy: 0.7018 - val_loss: 0.5949 - val_accuracy: 0.7400\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5931 - accuracy: 0.7193 - val_loss: 0.5944 - val_accuracy: 0.7500\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6027 - accuracy: 0.7068 - val_loss: 0.5945 - val_accuracy: 0.7300\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5983 - accuracy: 0.7093 - val_loss: 0.5945 - val_accuracy: 0.7200\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5958 - accuracy: 0.7093 - val_loss: 0.5945 - val_accuracy: 0.7100\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5901 - accuracy: 0.7143 - val_loss: 0.5949 - val_accuracy: 0.7100\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5863 - accuracy: 0.7093 - val_loss: 0.5951 - val_accuracy: 0.7200\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5891 - accuracy: 0.7068 - val_loss: 0.5955 - val_accuracy: 0.7300\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5777 - accuracy: 0.7268 - val_loss: 0.5958 - val_accuracy: 0.7300\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5853 - accuracy: 0.7068 - val_loss: 0.5962 - val_accuracy: 0.7600\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5936 - accuracy: 0.7243 - val_loss: 0.5968 - val_accuracy: 0.7600\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5753 - accuracy: 0.7368 - val_loss: 0.5955 - val_accuracy: 0.7600\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5778 - accuracy: 0.7268 - val_loss: 0.5954 - val_accuracy: 0.7600\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5821 - accuracy: 0.6967 - val_loss: 0.5954 - val_accuracy: 0.7400\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5819 - accuracy: 0.7218 - val_loss: 0.5960 - val_accuracy: 0.7400\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5799 - accuracy: 0.7268 - val_loss: 0.5946 - val_accuracy: 0.7400\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5777 - accuracy: 0.7068 - val_loss: 0.5943 - val_accuracy: 0.7400\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5669 - accuracy: 0.7118 - val_loss: 0.5935 - val_accuracy: 0.7400\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5814 - accuracy: 0.7243 - val_loss: 0.5945 - val_accuracy: 0.7400\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5673 - accuracy: 0.7268 - val_loss: 0.5948 - val_accuracy: 0.7400\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5725 - accuracy: 0.7293 - val_loss: 0.5954 - val_accuracy: 0.7400\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5666 - accuracy: 0.7093 - val_loss: 0.5952 - val_accuracy: 0.7400\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5631 - accuracy: 0.7243 - val_loss: 0.5955 - val_accuracy: 0.7400\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5879 - accuracy: 0.6992 - val_loss: 0.5951 - val_accuracy: 0.7400\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5815 - accuracy: 0.7168 - val_loss: 0.5944 - val_accuracy: 0.7400\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5582 - accuracy: 0.7444 - val_loss: 0.5940 - val_accuracy: 0.7400\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5867 - accuracy: 0.6817 - val_loss: 0.5941 - val_accuracy: 0.7400\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5698 - accuracy: 0.7293 - val_loss: 0.5922 - val_accuracy: 0.7300\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5606 - accuracy: 0.7118 - val_loss: 0.5913 - val_accuracy: 0.7300\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5690 - accuracy: 0.7268 - val_loss: 0.5925 - val_accuracy: 0.7300\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5673 - accuracy: 0.7168 - val_loss: 0.5923 - val_accuracy: 0.7400\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.5597 - accuracy: 0.7193 - val_loss: 0.5914 - val_accuracy: 0.7300\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5586 - accuracy: 0.7193 - val_loss: 0.5913 - val_accuracy: 0.7300\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5494 - accuracy: 0.7419 - val_loss: 0.5917 - val_accuracy: 0.7300\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5696 - accuracy: 0.7043 - val_loss: 0.5902 - val_accuracy: 0.7200\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5668 - accuracy: 0.7068 - val_loss: 0.5901 - val_accuracy: 0.7000\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5647 - accuracy: 0.7043 - val_loss: 0.5887 - val_accuracy: 0.7000\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5598 - accuracy: 0.7218 - val_loss: 0.5883 - val_accuracy: 0.7000\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5638 - accuracy: 0.7068 - val_loss: 0.5884 - val_accuracy: 0.7000\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5563 - accuracy: 0.7243 - val_loss: 0.5874 - val_accuracy: 0.7000\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5488 - accuracy: 0.7293 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5658 - accuracy: 0.7343 - val_loss: 0.5854 - val_accuracy: 0.7000\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5630 - accuracy: 0.7093 - val_loss: 0.5856 - val_accuracy: 0.7000\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5558 - accuracy: 0.7318 - val_loss: 0.5843 - val_accuracy: 0.7000\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5517 - accuracy: 0.7118 - val_loss: 0.5831 - val_accuracy: 0.7100\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5636 - accuracy: 0.7218 - val_loss: 0.5841 - val_accuracy: 0.7000\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5553 - accuracy: 0.7293 - val_loss: 0.5824 - val_accuracy: 0.7300\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5386 - accuracy: 0.7393 - val_loss: 0.5850 - val_accuracy: 0.7100\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5508 - accuracy: 0.7343 - val_loss: 0.5840 - val_accuracy: 0.7300\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5531 - accuracy: 0.7268 - val_loss: 0.5855 - val_accuracy: 0.7000\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5312 - accuracy: 0.7444 - val_loss: 0.5859 - val_accuracy: 0.7000\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5581 - accuracy: 0.7143 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5521 - accuracy: 0.7093 - val_loss: 0.5870 - val_accuracy: 0.7000\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5488 - accuracy: 0.7268 - val_loss: 0.5866 - val_accuracy: 0.7000\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5368 - accuracy: 0.7243 - val_loss: 0.5878 - val_accuracy: 0.7100\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5683 - accuracy: 0.7043 - val_loss: 0.5858 - val_accuracy: 0.7100\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5558 - accuracy: 0.7243 - val_loss: 0.5857 - val_accuracy: 0.7100\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5655 - accuracy: 0.6992 - val_loss: 0.5839 - val_accuracy: 0.7100\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5643 - accuracy: 0.7043 - val_loss: 0.5824 - val_accuracy: 0.7100\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5436 - accuracy: 0.7318 - val_loss: 0.5821 - val_accuracy: 0.7100\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5477 - accuracy: 0.7093 - val_loss: 0.5836 - val_accuracy: 0.7300\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5310 - accuracy: 0.7444 - val_loss: 0.5841 - val_accuracy: 0.7100\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5227 - accuracy: 0.7469 - val_loss: 0.5831 - val_accuracy: 0.7100\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5327 - accuracy: 0.7419 - val_loss: 0.5817 - val_accuracy: 0.7200\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5549 - accuracy: 0.7168 - val_loss: 0.5819 - val_accuracy: 0.7300\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5312 - accuracy: 0.7444 - val_loss: 0.5835 - val_accuracy: 0.7300\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5269 - accuracy: 0.7368 - val_loss: 0.5826 - val_accuracy: 0.7300\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5642 - accuracy: 0.7168 - val_loss: 0.5795 - val_accuracy: 0.7200\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5289 - accuracy: 0.7243 - val_loss: 0.5814 - val_accuracy: 0.7300\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5455 - accuracy: 0.7143 - val_loss: 0.5813 - val_accuracy: 0.7200\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5645 - accuracy: 0.6867 - val_loss: 0.5814 - val_accuracy: 0.7200\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5346 - accuracy: 0.7293 - val_loss: 0.5801 - val_accuracy: 0.7200\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5357 - accuracy: 0.7368 - val_loss: 0.5797 - val_accuracy: 0.7300\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5277 - accuracy: 0.7419 - val_loss: 0.5815 - val_accuracy: 0.7300\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.5337 - accuracy: 0.7218 - val_loss: 0.5830 - val_accuracy: 0.7300\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5288 - accuracy: 0.7469 - val_loss: 0.5823 - val_accuracy: 0.7300\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5227 - accuracy: 0.7343 - val_loss: 0.5805 - val_accuracy: 0.7300\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5494 - accuracy: 0.7193 - val_loss: 0.5810 - val_accuracy: 0.7400\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5294 - accuracy: 0.7293 - val_loss: 0.5789 - val_accuracy: 0.7400\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5380 - accuracy: 0.7393 - val_loss: 0.5798 - val_accuracy: 0.7300\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5317 - accuracy: 0.7268 - val_loss: 0.5823 - val_accuracy: 0.7300\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5526 - accuracy: 0.7068 - val_loss: 0.5805 - val_accuracy: 0.7400\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5425 - accuracy: 0.7018 - val_loss: 0.5822 - val_accuracy: 0.7400\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5225 - accuracy: 0.7569 - val_loss: 0.5851 - val_accuracy: 0.7300\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5225 - accuracy: 0.7419 - val_loss: 0.5855 - val_accuracy: 0.7300\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5229 - accuracy: 0.7419 - val_loss: 0.5876 - val_accuracy: 0.7300\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5408 - accuracy: 0.7093 - val_loss: 0.5865 - val_accuracy: 0.7300\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5243 - accuracy: 0.7469 - val_loss: 0.5859 - val_accuracy: 0.7300\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5358 - accuracy: 0.7293 - val_loss: 0.5819 - val_accuracy: 0.7300\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5153 - accuracy: 0.7318 - val_loss: 0.5792 - val_accuracy: 0.7300\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5098 - accuracy: 0.7368 - val_loss: 0.5797 - val_accuracy: 0.7400\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5397 - accuracy: 0.7018 - val_loss: 0.5860 - val_accuracy: 0.7400\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5258 - accuracy: 0.7268 - val_loss: 0.5861 - val_accuracy: 0.7300\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5386 - accuracy: 0.7293 - val_loss: 0.5856 - val_accuracy: 0.7300\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5182 - accuracy: 0.7544 - val_loss: 0.5862 - val_accuracy: 0.7300\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5337 - accuracy: 0.7419 - val_loss: 0.5826 - val_accuracy: 0.7400\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5262 - accuracy: 0.7393 - val_loss: 0.5803 - val_accuracy: 0.7500\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5502 - accuracy: 0.7293 - val_loss: 0.5824 - val_accuracy: 0.7600\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5420 - accuracy: 0.7293 - val_loss: 0.5826 - val_accuracy: 0.7700\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5133 - accuracy: 0.7444 - val_loss: 0.5863 - val_accuracy: 0.7700\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5059 - accuracy: 0.7619 - val_loss: 0.5882 - val_accuracy: 0.7700\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5229 - accuracy: 0.7318 - val_loss: 0.5868 - val_accuracy: 0.7700\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5258 - accuracy: 0.7093 - val_loss: 0.5885 - val_accuracy: 0.7700\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5438 - accuracy: 0.7168 - val_loss: 0.5871 - val_accuracy: 0.7700\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5049 - accuracy: 0.7494 - val_loss: 0.5916 - val_accuracy: 0.7600\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5125 - accuracy: 0.7544 - val_loss: 0.5957 - val_accuracy: 0.7700\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5336 - accuracy: 0.7218 - val_loss: 0.5935 - val_accuracy: 0.7600\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5070 - accuracy: 0.7569 - val_loss: 0.5903 - val_accuracy: 0.7500\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5085 - accuracy: 0.7444 - val_loss: 0.5891 - val_accuracy: 0.7300\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5378 - accuracy: 0.7343 - val_loss: 0.5854 - val_accuracy: 0.7400\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5261 - accuracy: 0.7318 - val_loss: 0.5842 - val_accuracy: 0.7400\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5043 - accuracy: 0.7594 - val_loss: 0.5878 - val_accuracy: 0.7400\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5111 - accuracy: 0.7318 - val_loss: 0.5937 - val_accuracy: 0.7400\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5355 - accuracy: 0.7218 - val_loss: 0.5987 - val_accuracy: 0.7400\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5139 - accuracy: 0.7293 - val_loss: 0.5973 - val_accuracy: 0.7300\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5099 - accuracy: 0.7544 - val_loss: 0.5990 - val_accuracy: 0.7300\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5162 - accuracy: 0.7444 - val_loss: 0.5985 - val_accuracy: 0.7400\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4973 - accuracy: 0.7393 - val_loss: 0.5955 - val_accuracy: 0.7500\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5000 - accuracy: 0.7469 - val_loss: 0.5968 - val_accuracy: 0.7500\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5035 - accuracy: 0.7393 - val_loss: 0.5972 - val_accuracy: 0.7500\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5097 - accuracy: 0.7243 - val_loss: 0.6003 - val_accuracy: 0.7600\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4894 - accuracy: 0.7619 - val_loss: 0.5998 - val_accuracy: 0.7500\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5155 - accuracy: 0.7494 - val_loss: 0.5954 - val_accuracy: 0.7600\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5013 - accuracy: 0.7494 - val_loss: 0.5931 - val_accuracy: 0.7700\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5135 - accuracy: 0.7218 - val_loss: 0.5983 - val_accuracy: 0.7800\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4911 - accuracy: 0.7519 - val_loss: 0.5952 - val_accuracy: 0.7600\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5083 - accuracy: 0.7519 - val_loss: 0.5974 - val_accuracy: 0.7500\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4945 - accuracy: 0.7519 - val_loss: 0.5978 - val_accuracy: 0.7800\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5204 - accuracy: 0.7419 - val_loss: 0.6018 - val_accuracy: 0.6100\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4946 - accuracy: 0.7519 - val_loss: 0.6046 - val_accuracy: 0.6100\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5025 - accuracy: 0.7494 - val_loss: 0.6082 - val_accuracy: 0.6100\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5056 - accuracy: 0.7444 - val_loss: 0.6142 - val_accuracy: 0.6100\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5229 - accuracy: 0.7068 - val_loss: 0.6086 - val_accuracy: 0.6100\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5045 - accuracy: 0.7293 - val_loss: 0.6050 - val_accuracy: 0.6100\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5181 - accuracy: 0.7494 - val_loss: 0.6046 - val_accuracy: 0.6100\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5401 - accuracy: 0.7268 - val_loss: 0.6037 - val_accuracy: 0.6100\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4996 - accuracy: 0.7368 - val_loss: 0.6013 - val_accuracy: 0.5900\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5042 - accuracy: 0.7343 - val_loss: 0.6003 - val_accuracy: 0.6100\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5019 - accuracy: 0.7343 - val_loss: 0.5997 - val_accuracy: 0.6100\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5017 - accuracy: 0.7519 - val_loss: 0.6011 - val_accuracy: 0.6100\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5096 - accuracy: 0.7594 - val_loss: 0.6000 - val_accuracy: 0.6100\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5241 - accuracy: 0.7293 - val_loss: 0.5965 - val_accuracy: 0.6100\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4922 - accuracy: 0.7569 - val_loss: 0.5981 - val_accuracy: 0.6100\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4708 - accuracy: 0.7594 - val_loss: 0.6048 - val_accuracy: 0.6100\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4964 - accuracy: 0.7694 - val_loss: 0.6112 - val_accuracy: 0.6200\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4955 - accuracy: 0.7444 - val_loss: 0.6095 - val_accuracy: 0.6200\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5251 - accuracy: 0.7368 - val_loss: 0.6040 - val_accuracy: 0.6200\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5041 - accuracy: 0.7243 - val_loss: 0.5972 - val_accuracy: 0.7900\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5185 - accuracy: 0.7393 - val_loss: 0.5946 - val_accuracy: 0.7900\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4861 - accuracy: 0.7719 - val_loss: 0.5981 - val_accuracy: 0.7900\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5059 - accuracy: 0.7544 - val_loss: 0.6054 - val_accuracy: 0.6100\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5156 - accuracy: 0.7268 - val_loss: 0.6080 - val_accuracy: 0.6000\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5337 - accuracy: 0.7193 - val_loss: 0.5983 - val_accuracy: 0.7700\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5158 - accuracy: 0.7519 - val_loss: 0.5941 - val_accuracy: 0.7600\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4847 - accuracy: 0.7694 - val_loss: 0.5963 - val_accuracy: 0.7700\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5080 - accuracy: 0.7368 - val_loss: 0.5992 - val_accuracy: 0.7700\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4748 - accuracy: 0.7644 - val_loss: 0.6079 - val_accuracy: 0.5900\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4907 - accuracy: 0.7569 - val_loss: 0.6140 - val_accuracy: 0.5900\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5275 - accuracy: 0.7368 - val_loss: 0.6082 - val_accuracy: 0.5900\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4955 - accuracy: 0.7569 - val_loss: 0.6120 - val_accuracy: 0.6000\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4758 - accuracy: 0.7845 - val_loss: 0.6095 - val_accuracy: 0.5900\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5028 - accuracy: 0.7469 - val_loss: 0.6142 - val_accuracy: 0.5900\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4706 - accuracy: 0.7519 - val_loss: 0.6140 - val_accuracy: 0.5900\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5241 - accuracy: 0.7293 - val_loss: 0.6199 - val_accuracy: 0.5900\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5167 - accuracy: 0.7293 - val_loss: 0.6160 - val_accuracy: 0.5900\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4921 - accuracy: 0.7669 - val_loss: 0.6153 - val_accuracy: 0.5800\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4899 - accuracy: 0.7594 - val_loss: 0.6105 - val_accuracy: 0.5900\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4700 - accuracy: 0.7669 - val_loss: 0.6137 - val_accuracy: 0.5900\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4961 - accuracy: 0.7444 - val_loss: 0.6069 - val_accuracy: 0.6000\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4674 - accuracy: 0.7619 - val_loss: 0.6112 - val_accuracy: 0.6100\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5017 - accuracy: 0.7368 - val_loss: 0.6109 - val_accuracy: 0.6000\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4889 - accuracy: 0.7519 - val_loss: 0.6129 - val_accuracy: 0.6000\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4786 - accuracy: 0.7644 - val_loss: 0.6051 - val_accuracy: 0.7500\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4595 - accuracy: 0.7519 - val_loss: 0.6048 - val_accuracy: 0.7500\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4980 - accuracy: 0.7519 - val_loss: 0.6039 - val_accuracy: 0.7500\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4838 - accuracy: 0.7393 - val_loss: 0.6093 - val_accuracy: 0.5800\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5117 - accuracy: 0.7494 - val_loss: 0.6087 - val_accuracy: 0.5900\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(X,y,epochs=200,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "5Tl3oLyIXR3Q",
    "outputId": "1efee8ad-4da1-493e-f25d-09d2076701c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f5348cb010>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['val_accuracy'],color='black')\n",
    "plt.plot(history2.history['val_accuracy'],color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3c-OkF9XoTf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginner Level:\n",
    "\n",
    "1. **What is batch normalization, and why is it used in neural networks?**\n",
    "   Batch normalization is a technique used to improve the training of deep neural networks by normalizing the input of each layer. It involves normalizing the activations of each layer by subtracting the batch mean and dividing by the batch standard deviation. Batch normalization helps address the internal covariate shift problem, stabilizes the training process, and accelerates convergence.\n",
    "\n",
    "2. **Explain the concept of internal covariate shift and how batch normalization addresses it.**\n",
    "   Internal covariate shift refers to the change in the distribution of network activations as the parameters of the network are updated during training. This can slow down the training process and make it difficult to optimize deep neural networks. Batch normalization addresses internal covariate shift by normalizing the activations of each layer, ensuring that they have zero mean and unit variance, which helps stabilize the training process.\n",
    "\n",
    "3. **How does batch normalization work during training and inference phases?**\n",
    "   During training, batch normalization computes the mean and standard deviation of the activations within each mini-batch and normalizes the activations using these statistics. During inference, batch normalization typically uses the running mean and running standard deviation computed during training to normalize the activations.\n",
    "\n",
    "### Intermediate Level:\n",
    "\n",
    "4. **Can you explain the mathematical formulation of batch normalization?**\n",
    "   Batch normalization normalizes the activations of each layer by subtracting the batch mean and dividing by the batch standard deviation. Mathematically, the normalized activations \\( \\hat{x} \\) are calculated as follows:\n",
    "   \\[ \\hat{x} = \\frac{x - \\text{mean}(x)}{\\sqrt{\\text{var}(x) + \\epsilon}} \\]\n",
    "   where \\( x \\) is the input to the layer, \\( \\text{mean}(x) \\) is the mean of the batch, \\( \\text{var}(x) \\) is the variance of the batch, and \\( \\epsilon \\) is a small constant to prevent division by zero.\n",
    "\n",
    "5. **How does batch normalization affect the optimization landscape of neural networks?**\n",
    "   Batch normalization smooths the optimization landscape of neural networks by reducing the internal covariate shift and stabilizing the training process. This can lead to faster convergence, improved gradient flow, and reduced vanishing or exploding gradients, making it easier to train deep networks.\n",
    "\n",
    "6. **Can batch normalization be applied to all layers of a neural network? If not, why?**\n",
    "   Batch normalization can be applied to all layers of a neural network, including convolutional layers, fully connected layers, and recurrent layers. However, it may not always be beneficial to apply batch normalization to all layers, especially in deeper networks or in certain architectures such as generative adversarial networks (GANs), where batch normalization may interfere with the learning dynamics.\n",
    "\n",
    "### Advanced Level:\n",
    "\n",
    "7. **How does batch normalization interact with other regularization techniques such as dropout?**\n",
    "   Batch normalization acts as a form of regularization by reducing internal covariate shift and stabilizing the training process. In some cases, batch normalization may reduce the need for other regularization techniques such as dropout. However, batch normalization and dropout can be used together synergistically to further improve the generalization performance of neural networks.\n",
    "\n",
    "8. **What are some alternatives to batch normalization, and how do they differ from each other?**\n",
    "   Some alternatives to batch normalization include layer normalization, instance normalization, and group normalization. These techniques differ in how they normalize the activations of each layer and the context in which they are applied.\n",
    "\n",
    "9. **Can you describe any recent advancements or research trends in batch normalization or related techniques for improving training stability and convergence in neural networks?**\n",
    "   Recent advancements in batch normalization include techniques such as batch renormalization, which aims to improve the stability of batch normalization during training, and adaptive normalization methods, which adaptively adjust the normalization parameters based on the data distribution or the network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **What is batch normalization, and why is it used in neural networks?**\n",
    "Batch normalization is a technique used to improve the training of deep neural networks by normalizing the input of each layer. It involves normalizing the activations of each layer by subtracting the batch mean and dividing by the batch standard deviation. Batch normalization helps address the internal covariate shift problem, stabilizes the training process, and accelerates convergence.\n",
    "\n",
    "### 2. **Explain the concept of internal covariate shift and how batch normalization addresses it.**\n",
    "Internal covariate shift refers to the change in the distribution of network activations as the parameters of the network are updated during training. This can slow down the training process and make it difficult to optimize deep neural networks. Batch normalization addresses internal covariate shift by normalizing the activations of each layer, ensuring that they have zero mean and unit variance, which helps stabilize the training process.\n",
    "\n",
    "### 3. **How does batch normalization work during training and inference phases?**\n",
    "During training, batch normalization computes the mean and standard deviation of the activations within each mini-batch and normalizes the activations using these statistics. During inference, batch normalization typically uses the running mean and running standard deviation computed during training to normalize the activations.\n",
    "\n",
    "### 4. **What are the key benefits of using batch normalization?**\n",
    "Some key benefits of batch normalization include:\n",
    "- Accelerated training: Batch normalization helps stabilize the training process, allowing for faster convergence and reduced training time.\n",
    "- Improved generalization: Batch normalization acts as a form of regularization, reducing the need for other regularization techniques such as dropout.\n",
    "- Robustness to initialization: Batch normalization reduces the sensitivity of neural networks to the choice of initialization parameters, making it easier to train deep networks.\n",
    "\n",
    "### 5. **What are some potential drawbacks or limitations of batch normalization?**\n",
    "Some potential drawbacks or limitations of batch normalization include:\n",
    "- Increased memory usage: Batch normalization requires storing additional statistics for each layer, increasing the memory footprint of the model.\n",
    "- Reduced effectiveness with small batch sizes: Batch normalization may not work well with very small batch sizes, as the batch statistics may be less representative of the true population statistics.\n",
    "- Dependency on mini-batch statistics: Batch normalization relies on mini-batch statistics, which may introduce noise and instability, especially in the early stages of training.\n",
    "\n",
    "### 6. **How does batch normalization affect the optimization landscape of neural networks?**\n",
    "Batch normalization smooths the optimization landscape of neural networks by reducing the internal covariate shift and stabilizing the training process. This can lead to faster convergence, improved gradient flow, and reduced vanishing or exploding gradients, making it easier to train deep networks.\n",
    "\n",
    "### 7. **Can batch normalization be applied to all layers of a neural network? If not, why?**\n",
    "Batch normalization can be applied to all layers of a neural network, including convolutional layers, fully connected layers, and recurrent layers. However, it may not always be beneficial to apply batch normalization to all layers, especially in deeper networks or in certain architectures such as generative adversarial networks (GANs), where batch normalization may interfere with the learning dynamics.\n",
    "\n",
    "### 8. **How does batch normalization interact with other regularization techniques such as dropout?**\n",
    "Batch normalization acts as a form of regularization by reducing internal covariate shift and stabilizing the training process. In some cases, batch normalization may reduce the need for other regularization techniques such as dropout. However, batch normalization and dropout can be used together synergistically to further improve the generalization performance of neural networks.\n",
    "\n",
    "### 9. **What are some alternatives to batch normalization?**\n",
    "Some alternatives to batch normalization include:\n",
    "- Layer normalization: Normalizes the activations of each layer across the entire mini-batch, rather than separately for each feature.\n",
    "- Instance normalization: Normalizes the activations of each channel across spatial locations within each sample, rather than across the entire mini-batch.\n",
    "- Group normalization: Divides the channels of each sample into groups and normalizes the activations within each group separately.\n",
    "\n",
    "### 10. **How does batch normalization affect the distribution of gradients during backpropagation?**\n",
    "Batch normalization can help mitigate the vanishing and exploding gradient problem by stabilizing the training process and improving gradient flow. By normalizing the activations of each layer, batch normalization ensures that gradients are more consistent and less likely to vanish or explode as they propagate through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
