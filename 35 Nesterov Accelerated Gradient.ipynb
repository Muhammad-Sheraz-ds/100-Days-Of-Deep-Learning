{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "\n",
    "\n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Deep Learning</h1>\n",
    "\n",
    "---\n",
    "<h3 align=\"right\">Muhammad Sheraz (Data Scientist)</h3>\n",
    "<h1 align=\"center\">Day35 (Nesterov Accelerated Gradient)</h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src='Images/mom_nag.png'> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12'>Read here For Detail Lecture</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nesterov Accelerated Gradient Descent\n",
    "\n",
    "<img width='50%' align='right' src='Images/mom_nag.png'> \n",
    "\n",
    "- A variant of the gradient descent optimization algorithm.\n",
    "- Extension of basic gradient descent.\n",
    "- Utilizes `momentum` for faster convergence.\n",
    "  \n",
    "\n",
    "**Intution:**\n",
    "\n",
    "- Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we’ll be using γV(t−1) for modifying the weights so, θ−γV(t−1) approximately tells us the future location.\n",
    "\n",
    "\n",
    "<img src='Images/f_nag.PNG'>\n",
    "\n",
    "\n",
    "**Usage**:\n",
    "- Particularly useful in training *deep neural networks*.\n",
    "- Faster convergence compared to basic gradient descent or momentum-based methods.\n",
    "- Helps mitigate oscillations, leading to more stable convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "- Does not miss the `local minima.`\n",
    "- `Slows` if minima’s are occurring.\n",
    "\n",
    "### Disadvantages:\n",
    "- It can stuck in `local minima`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Difference Between Momentum and Nesterov Accelerated Gradient Descent </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                              | Momentum                                    | Nesterov Accelerated Gradient Descent (NAG)                                      |\n",
    "|--------------------------------------|---------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| Basis                                | Basic gradient descent with momentum       | Extension of momentum-based optimization with lookahead step                      |\n",
    "| Update Rule                          | Standard momentum update rule              | Modified update rule considering the lookahead position                           |\n",
    "| Calculation of Gradient              | Gradient calculated at current position    | Gradient calculated at lookahead position                                         |\n",
    "| Anticipation                         | No anticipation of future direction        | Anticipates future direction by adjusting the lookahead position with momentum    |\n",
    "| Convergence Improvement              | Helps in accelerating convergence          | Further improves convergence by reducing overshooting                             |\n",
    "| Application                          | Widely used in various optimization tasks  | Particularly useful in training deep neural networks for faster convergence        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Initialize Parameters**: Initialize the parameters (weights and biases) of your model randomly or using some predetermined values.\n",
    "\n",
    "2. **Set Hyperparameters**: Determine the hyperparameters such as the learning rate (`α`) and momentum (`μ`).\n",
    "\n",
    "3. **Initialize Velocity**: Initialize the velocity vector (`v_t`) to zero or a small random value.\n",
    "\n",
    "4. **Iterative Optimization**:\n",
    "   - **Forward Pass**: Perform a forward pass through your model to compute the loss function.\n",
    "   - **Calculate Gradient**: Calculate the gradient of the loss function with respect to the parameters.\n",
    "   - **Update Velocity with Momentum**: Update the velocity using the momentum term:\n",
    "     ```\n",
    "     v_t = μ * v_{t-1} - α * ∇f(x_{t-1} + μ * v_{t-1})\n",
    "     ```\n",
    "   - **Lookahead Update**: Calculate the lookahead position:\n",
    "     ```\n",
    "     x_{lookahead} = x_{t-1} + μ * v_{t}\n",
    "     ```\n",
    "   - **Calculate Gradient at Lookahead**: Calculate the gradient of the loss function with respect to the parameters at the lookahead position.\n",
    "   - **Update Parameters**: Update the parameters using the gradient at the lookahead position:\n",
    "     ```\n",
    "     x_t = x_{t-1} - α * ∇f(x_{lookahead})\n",
    "     ```\n",
    "\n",
    "5. **Repeat**: Repeat the iterative optimization process until convergence criteria are met (e.g., the loss function stops decreasing or reaches a threshold).\n",
    "\n",
    "6. **Convergence Check**: Monitor the convergence of the optimization process by tracking the value of the loss function or other performance metrics on a validation set.\n",
    "\n",
    "7. **Evaluate**: After convergence or upon reaching a stopping criterion, evaluate the performance of your model on a separate test set to assess its generalization ability.\n",
    "\n",
    "8. **Adjust Hyperparameters**: Fine-tune the hyperparameters if necessary based on the performance evaluation results.\n",
    "\n",
    "9. **Deploy**: Once satisfied with the model's performance, deploy it for making predictions on new data.\n",
    "\n",
    "10. **Regularization (Optional)**: Implement regularization techniques such as L1 or L2 regularization to prevent overfitting if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [-4.32356380e+33 -5.47672654e+33]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nag_optimizer(X, y, initial_params, learning_rate, momentum, num_iterations):\n",
    "    params = initial_params\n",
    "    velocity = np.zeros_like(params)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate gradient at lookahead position\n",
    "        lookahead_params = params - momentum * velocity\n",
    "        gradient = compute_gradient(X, y, lookahead_params)\n",
    "        \n",
    "        # Update velocity\n",
    "        velocity = momentum * velocity - learning_rate * gradient\n",
    "        \n",
    "        # Update parameters\n",
    "        params += velocity\n",
    "        \n",
    "        # Optionally, you can include convergence criteria here\n",
    "        # Example: if np.linalg.norm(gradient) < tolerance:\n",
    "        #              break\n",
    "    \n",
    "    return params\n",
    "\n",
    "def compute_gradient(X, y, params):\n",
    "    # Compute gradient of loss function with respect to parameters\n",
    "    # This is specific to your model and loss function\n",
    "    # For simplicity, assuming a linear regression model here\n",
    "    predictions = np.dot(X, params)\n",
    "    errors = predictions - y\n",
    "    gradient = np.dot(X.T, errors) / len(X)\n",
    "    return gradient\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])  # Input features\n",
    "y = np.array([3, 4, 5])  # Target labels\n",
    "initial_params = np.zeros(X.shape[1])  # Initialize parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run Nesterov Accelerated Gradient Descent\n",
    "final_params = nag_optimizer(X, y, initial_params, learning_rate, momentum, num_iterations)\n",
    "print(\"Final parameters:\", final_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Interview Questions</h1>\n",
    "\n",
    "\r\n",
    "### What is Nesterov Accelerated Gradient Descent?\r\n",
    "\r\n",
    "Nesterov Accelerated Gradient Descent (NAG) is a variant of the gradient descent optimization algorithm that helps accelerate convergence by modifying the update rule to take into account the momentum.\r\n",
    "\r\n",
    "### How does NAG differ from standard momentum-based optimization?\r\n",
    "\r\n",
    "NAG calculates the gradient not at the current position but at a lookahead position, which is adjusted using the momentum term. This lookahead update allows the algorithm to anticipate the future direction of movement, helping to reduce overshooting and improve convergence speed, especially in narrow va### Why is Nesterov Accelerated Gradient Descent useful?\r\n",
    "\r\n",
    "NAG further refines the update rule by considering the lookahead position, which helps to reduce overshooting and improve convergence, especially in scenarios with complex optimization landscapes. It accelerates convergence, particularly in scenarios with long, narrow valleys.\r\n",
    "\r\n",
    "### When would you choose to use Nesterov Accelerated Gradient Descent over standard gradient descent?\r\n",
    "\r\n",
    "NAG is particularly useful in training deep neural networks, where it often converges faster than the standard gradient descent algorithm or its variants like momentum-based gradient descent. It helps mitigate the issue of oscillations typically seen in momentum-based methods, which can result in overshooting the optimal solution.\r\n",
    "as follows:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
