{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "<img align=\"left\" width=\"110\"   src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\"> \n",
    "\n",
    "\n",
    "<h1 align=\"center\">Tools and Techniques for Data Science</h1>\n",
    "<h1 align=\"center\">Course: Deep Learning</h1>\n",
    "\n",
    "---\n",
    "<h3 align=\"right\">Muhammad Sheraz (Data Scientist)</h3>\n",
    "<h1 align=\"center\">Day36 (AdaGrad Optimization Algorithm)</h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src='Images/ada_grad.jpg'> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://medium.com/@brijesh_soni/understanding-the-adagrad-optimization-algorithm-an-adaptive-learning-rate-approach-9dfaae2077bb#:~:text=AdaGrad%20adjusts%20the%20learning%20rate,and%20variables%20of%20different%20sizes.'>Read here For Detail Lecture</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AdaGrad Algorithm\n",
    "\n",
    "<img width='50%' align='right' src='Images/ada_grad.jpg'>\n",
    "\n",
    "- AdaGrad (short for Adaptive Gradient Algorithm) is a popular optimization algorithm commonly used in machine learning and deep learning.\n",
    "-  It is designed to adaptively adjust the learning rate during training, which can be particularly useful when dealing with sparse data.\n",
    "\n",
    "- The key idea behind AdaGrad is to scale the learning rate for each parameter based on the historical gradients for that parameter. It achieves this by dividing the learning rate by the square root of the sum of the squares of the historical gradients. This means that parameters that have been updated frequently will have a smaller learning rate, while parameters that have been updated infrequently will have a larger learning rate.\n",
    "\n",
    "<img src='Images/FA_ADGRADE.png'>\n",
    "\n",
    "\n",
    "\n",
    "#### How AdaGrad Works\r\n",
    "\r\n",
    "AdaGrad’s main principle is to scale the learning rate for each parameter according to the total of squared gradients observed during training.\r\n",
    "\r",
    "#\n",
    "### Steps of the Algorithm:\r\n",
    "\r\n",
    "#### Step 1: Initialize Variables\r\n",
    "\r\n",
    "- Initialize the parameters θ and a small constant ϵ to avoid division by zero.\r\n",
    "- Initialize the sum of squared gradients variable G with zeros, which has the same shape as θ.\r\n",
    "\r\n",
    "#### Step 2: Calculate Gradients\r\n",
    "\r\n",
    "- Compute the gradient of the loss function with respect to each parameter, ∇θJ(θ).\r\n",
    "\r\n",
    "#### Step 3: Accumulate Squared Gradients\r\n",
    "\r\n",
    "- Update the sum of squared gradients G for each parameter i: G[i] += (∇θJ(θ[i]))².\r\n",
    "\r\n",
    "#### Step 4: Update Parameters\r\n",
    "\r\n",
    "- Update each parameter using the adaptive learning rate: θ[i] -= (η / (√(G[i]) + ϵ)) * ∇θJ(θ[i]).\r\n",
    "\r\n",
    "In the above equations, η denotes the learning rate, and ∇θJ(θ[i]) represents the gradient of the loss function with respect to parameter θ[i].\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of AdaGrad Algorithm\r\n",
    "\r\n",
    "### Advantages\r\n",
    "\r\n",
    "1. **Adaptive Learning Rate**: AdaGrad adapts the learning rate for each parameter based on the historical gradients. This allows it to perform well in scenarios with sparse data or noisy gradients.\r\n",
    "\r\n",
    "2. **Automatic Scaling**: The algorithm automatically scales the learning rates based on the magnitude of the gradients. This can lead to faster convergence and better performance compared to fixed learning rate methods.\r\n",
    "\r\n",
    "3. **Efficient for Sparse Data**: AdaGrad's ability to handle sparse data makes it suitable for tasks such as natural language processing where feature vectors are often sparse.\r\n",
    "\r\n",
    "4. **No Manual Tuning of Learning Rate**: AdaGrad eliminates the need for manual tuning of the learning rate hyperparameter, as it adapts the learning rates during training.\r\n",
    "\r\n",
    "### Disadvantages\r\n",
    "\r\n",
    "1. **Accumulation of Squared Gradients**: AdaGrad accumulates the squared gradients in the denominator, which can lead to overly small learning rates over time. This can result in premature convergence and slow down the learning process.\r\n",
    "\r\n",
    "2. **Memory Intensive**: Since AdaGrad keeps track of the sum of squared gradients for each parameter, it can be memory-intensive, especially when dealing with a large number of parameters.\r\n",
    "\r\n",
    "3. **Non-Convex Problems**: AdaGrad may struggle with non-convex optimization problems, as it may converge to suboptimal solutions due to its overly cautious learning rate updates.\r\n",
    "\r\n",
    "4. **Difficulty with Changing Data Distributions**: AdaGrad's adaptive learning rates may not adapt quickly to changes in the data distribution, which can be problematic in dynamic environments.\r\n",
    "\r\n",
    "Overall, while AdaGrad offers benefits such as adaptive learning rates and efficient handling of sparse data, it also has limitations, particularly concerning the accumulation of squared gradients and its performance on non-convex problems.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Interview Questions</h1>\n",
    "\n",
    "\r\n",
    "### What is Adagrad optimization?\r\n",
    "\r\n",
    "Adagrad, short for Adaptive Gradient Algorithm, is an optimization algorithm used in machine learning and deep learning. It adapts the learning rate for each parameter based on the historical gradients for that parameter. It is particularly useful for sparse data, where certain features may have very different frequencies.\r\n",
    "\r\n",
    "### How does Adagrad work?\r\n",
    "\r\n",
    "Adagrad works by scaling the learning rate for each parameter based on the sum of the squared gradients accumulated over time. Essentially, it gives larger updates to parameters that have received smaller updates in the past, and vice versa. This allows parameters that are infrequently updated to have larger learning rates, which can be beneficial for convergence.\r\n",
    "\r\n",
    "### What are the advantages of using Adagrad?\r\n",
    "\r\n",
    "1. **Automatic Learning Rate Adjustment:** Adagrad automatically adjusts the learning rates for different parameters based on their past gradients. This can lead to faster convergence, especially for sparse data.\r\n",
    "2. **Effective for Sparse Data:** Adagrad performs well on sparse data because it individually adapts the learning rates for each parameter based on its frequency of updates.\r\n",
    "3. **No Manual Tuning of Learning Rates:** Unlike some optimization algorithms where manual tuning of learning rates may be required, Adagrad adjusts the learning rates automatically, saving time and effort.\r\n",
    "\r\n",
    "### What are the limitations of Adagrad?\r\n",
    "\r\n",
    "1. **Accumulation of Squared Gradients:** Adagrad accumulates the squared gradients over time, which can result in very small learning rates for parameters that are updated frequently. This can cause the learning process to slow down significantly in later stages.\r\n",
    "2. **Not Suitable for Non-Convex Problems:** Adagrad may perform poorly on non-convex optimization problems, such as deep neural networks, due to the aggressive diminishing of learning rates, which can lead to premature convergence to suboptimal solutions.\r\n",
    "3. **Memory Requirement:** Adagrad requires storing the squared gradients for each parameter, which can be memory-intensive, especially when dealing with a large number of parameters.\r\n",
    "\r\n",
    "### How can the limitations of Adagrad be mitigated?\r\n",
    "\r\n",
    "1. **Adaptive Learning Rate Decay:** Implementing adaptive learning rate decay mechanisms can help mitigate the problem of overly aggressive learning rate reduction. Techniques like RMSprop and Adam are variants of Adagrad that address this issue.\r\n",
    "2. **Parameter Initialization:** Proper initialization of parameters can help alleviate some of the issues associated with Adagrad, particularly the accumulation of very small learning rates.\r\n",
    "3. **Exploration of Alternatives:** Exploring alternative optimization algorithms such as RMSprop, Adam, or AdaDelta, which address the limitations of Adagrad, can be beneficial, especially for complex deep learning models.\r\n",
    "\r\n",
    "as follows:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
